{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483bb009-7f77-4214-b7be-a2cacf5d27fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import Session, get_execution_role, clarify\n",
    "from sagemaker.deserializers import JSONDeserializer, StringDeserializer\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.model_monitor import (\n",
    "    CronExpressionGenerator,\n",
    "    DataCaptureConfig,\n",
    "    DefaultModelMonitor,\n",
    ")\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392623c-bbd0-48c2-965e-ca446657b2d6",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3734e58-b27c-4494-950b-2e3eb688668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "RAW_CSV   = \"star_classification.csv\"\n",
    "TRAIN_CSV = \"data/splits/train.csv\"\n",
    "TEST_CSV  = \"data/splits/test.csv\"\n",
    "PROD_CSV  = \"data/splits/prod_monitor.csv\"\n",
    "\n",
    "# label mapping (used for eval + optional post-processing)\n",
    "LABEL_TO_ID = {\"GALAXY\": 0, \"STAR\": 1, \"QSO\": 2}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_TO_ID.items()}\n",
    "\n",
    "# SageMaker/AWS \n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "boto_sess = boto3.Session(region_name=region)\n",
    "\n",
    "s3 = boto_sess.client(\"s3\")\n",
    "sm = boto_sess.client(\"sagemaker\")\n",
    "cw = boto_sess.client(\"cloudwatch\")\n",
    "\n",
    "sts = boto_sess.client(\"sts\")\n",
    "account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "BUCKET = f\"sagemaker-{region}-{account_id}\"\n",
    "PREFIX = \"sagemaker-featurestore-demo\"\n",
    "\n",
    "S3_BASE = f\"s3://{BUCKET}/{PREFIX}\"\n",
    "S3_RAW  = f\"{S3_BASE}/raw/\"\n",
    "S3_SPLITS = f\"{S3_BASE}/splits/\"\n",
    "S3_PREP = f\"{S3_BASE}/prepared/\"\n",
    "S3_OUT  = f\"{S3_BASE}/output/\"\n",
    "S3_MON  = f\"{S3_BASE}/monitoring/\"\n",
    "\n",
    "def ensure_bucket(bucket: str):\n",
    "    try:\n",
    "        s3.head_bucket(Bucket=bucket)\n",
    "    except Exception:\n",
    "        if region == \"us-east-1\":\n",
    "            s3.create_bucket(Bucket=bucket)\n",
    "        else:\n",
    "            s3.create_bucket(Bucket=bucket, CreateBucketConfiguration={\"LocationConstraint\": region})\n",
    "\n",
    "ensure_bucket(BUCKET)\n",
    "\n",
    "def parse_s3_uri(uri: str):\n",
    "    p = urlparse(uri)\n",
    "    if p.scheme != \"s3\":\n",
    "        raise ValueError(f\"Not an s3 uri: {uri}\")\n",
    "    return p.netloc, p.path.lstrip(\"/\")\n",
    "\n",
    "def s3_upload(local_path: str, s3_uri: str):\n",
    "    b, k = parse_s3_uri(s3_uri)\n",
    "    s3.upload_file(local_path, b, k)\n",
    "    return s3_uri\n",
    "\n",
    "def s3_read_csv(s3_uri: str) -> pd.DataFrame:\n",
    "    b, k = parse_s3_uri(s3_uri)\n",
    "    obj = s3.get_object(Bucket=b, Key=k)\n",
    "    return pd.read_csv(BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "def s3_put_bytes(data: bytes, s3_uri: str, content_type: str):\n",
    "    b, k = parse_s3_uri(s3_uri)\n",
    "    s3.put_object(Bucket=b, Key=k, Body=data, ContentType=content_type)\n",
    "    return s3_uri\n",
    "\n",
    "def list_s3_objects(s3_prefix_uri: str):\n",
    "    b, pfx = parse_s3_uri(s3_prefix_uri)\n",
    "    if pfx and not pfx.endswith(\"/\"):\n",
    "        pfx += \"/\"\n",
    "    out = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": b, \"Prefix\": pfx, \"MaxKeys\": 1000}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        out.extend(resp.get(\"Contents\", []))\n",
    "        if not resp.get(\"IsTruncated\"):\n",
    "            break\n",
    "        token = resp.get(\"NextContinuationToken\")\n",
    "    return b, pfx, out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2a482f-d92e-448d-9a91-c7c4d2379c30",
   "metadata": {},
   "source": [
    "### Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f2bb7a-7c82-41b1-938d-2b7ac794090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local shapes:\n",
      "  train: (48000, 18)\n",
      "  test:  (12000, 18)\n",
      "  prod:  (40000, 18)\n"
     ]
    }
   ],
   "source": [
    "df_train_raw = pd.read_csv(TRAIN_CSV)\n",
    "df_test_raw  = pd.read_csv(TEST_CSV)\n",
    "df_prod_raw  = pd.read_csv(PROD_CSV)\n",
    "\n",
    "print(\"Local shapes:\")\n",
    "print(\"  train:\", df_train_raw.shape)\n",
    "print(\"  test: \", df_test_raw.shape)\n",
    "print(\"  prod: \", df_prod_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df15b76-b060-4ec2-aa5e-aff7c25f9e15",
   "metadata": {},
   "source": [
    "### Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a0d700-99a4-4913-93a0-a443aa1a778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded to S3:\n",
      "  raw:   s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/raw/star_classification.csv\n",
      "  train: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/splits/train.csv\n",
      "  test:  s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/splits/test.csv\n",
      "  prod:  s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/splits/prod_monitor.csv\n"
     ]
    }
   ],
   "source": [
    "S3_RAW_CSV   = f\"{S3_RAW}{os.path.basename(RAW_CSV)}\"\n",
    "S3_TRAIN_CSV = f\"{S3_SPLITS}train.csv\"\n",
    "S3_TEST_CSV  = f\"{S3_SPLITS}test.csv\"\n",
    "S3_PROD_CSV  = f\"{S3_SPLITS}prod_monitor.csv\"\n",
    "\n",
    "s3_upload(RAW_CSV,   S3_RAW_CSV)\n",
    "s3_upload(TRAIN_CSV, S3_TRAIN_CSV)\n",
    "s3_upload(TEST_CSV,  S3_TEST_CSV)\n",
    "s3_upload(PROD_CSV,  S3_PROD_CSV)\n",
    "\n",
    "print(\"Uploaded to S3:\")\n",
    "print(\"  raw:  \", S3_RAW_CSV)\n",
    "print(\"  train:\", S3_TRAIN_CSV)\n",
    "print(\"  test: \", S3_TEST_CSV)\n",
    "print(\"  prod: \", S3_PROD_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331867e-3ba0-49e2-bd49-606196e3b36f",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae326956-ee5d-4363-af24-f8ac1bda83b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered shapes:\n",
      "  train: (47999, 31)\n",
      "  test:  (12000, 31)\n",
      "  prod:  (40000, 31)\n",
      "Prepared training inputs:\n",
      "  feature_cols: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/prepared/20260217-215414/feature_cols.json\n",
      "  train: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/prepared/20260217-215414/train.csv\n",
      "  val:   s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/prepared/20260217-215414/validation.csv\n"
     ]
    }
   ],
   "source": [
    "def fe_pipeline(df: pd.DataFrame):\n",
    "    df2 = df.copy()\n",
    "\n",
    "    # Normalize expected numeric columns\n",
    "    bands = [\"u\", \"g\", \"r\", \"i\", \"z\"]\n",
    "    for c in bands + [\"alpha\", \"delta\", \"redshift\"]:\n",
    "        if c in df2.columns:\n",
    "            df2[c] = pd.to_numeric(df2[c], errors=\"coerce\")\n",
    "\n",
    "    # Common cleaning for sentinel/invalid values (adjust if you want)\n",
    "    for c in bands:\n",
    "        if c in df2.columns:\n",
    "            df2.loc[df2[c] <= -1000, c] = np.nan\n",
    "    if \"redshift\" in df2.columns:\n",
    "        df2.loc[df2[\"redshift\"] < -0.1, \"redshift\"] = np.nan\n",
    "\n",
    "    # Color indices (adjacent + remaining pairs)\n",
    "    adj_pairs = [(\"u\",\"g\"), (\"g\",\"r\"), (\"r\",\"i\"), (\"i\",\"z\")]\n",
    "    all_pairs = []\n",
    "    for i in range(len(bands)):\n",
    "        for j in range(i+1, len(bands)):\n",
    "            all_pairs.append((bands[i], bands[j]))\n",
    "\n",
    "    for b1, b2 in all_pairs:\n",
    "        if b1 in df2.columns and b2 in df2.columns:\n",
    "            df2[f\"{b1}_{b2}\"] = df2[b1] - df2[b2]\n",
    "\n",
    "    color_feats = [f\"{b1}_{b2}\" for (b1,b2) in all_pairs if f\"{b1}_{b2}\" in df2.columns]\n",
    "\n",
    "    # Summary stats\n",
    "    present_bands = [c for c in bands if c in df2.columns]\n",
    "    if present_bands:\n",
    "        df2[\"mean_mag\"] = df2[present_bands].mean(axis=1)\n",
    "        df2[\"mag_std\"]  = df2[present_bands].std(axis=1)\n",
    "        df2[\"mag_span\"] = df2[present_bands].max(axis=1) - df2[present_bands].min(axis=1)\n",
    "\n",
    "    # Minimal feature set\n",
    "    base_feats = [c for c in [\"alpha\", \"delta\"] + bands + [\"redshift\"] if c in df2.columns]\n",
    "    eng_feats  = color_feats + [c for c in [\"mean_mag\", \"mag_std\", \"mag_span\"] if c in df2.columns]\n",
    "    feature_cols = base_feats + eng_feats\n",
    "\n",
    "    # Require label + required numeric fields\n",
    "    required = [c for c in (bands + [\"redshift\"]) if c in df2.columns]\n",
    "    df2 = df2.dropna(subset=required + [\"class\"]).reset_index(drop=True)\n",
    "\n",
    "    return df2, feature_cols\n",
    "\n",
    "df_train_fe, feature_cols = fe_pipeline(df_train_raw)\n",
    "df_test_fe,  _            = fe_pipeline(df_test_raw)\n",
    "df_prod_fe,  _            = fe_pipeline(df_prod_raw)\n",
    "\n",
    "print(\"Engineered shapes:\")\n",
    "print(\"  train:\", df_train_fe.shape)\n",
    "print(\"  test: \", df_test_fe.shape)\n",
    "print(\"  prod: \", df_prod_fe.shape)\n",
    "\n",
    "# Write engineered datasets to S3\n",
    "def to_xgb_ready(df_fe: pd.DataFrame, feature_cols: list[str], label_to_id: dict):\n",
    "    y = df_fe[\"class\"].astype(str).str.strip().str.upper().map(label_to_id)\n",
    "    if y.isna().any():\n",
    "        bad = sorted(df_fe[\"class\"][y.isna()].astype(str).unique().tolist())\n",
    "        raise ValueError(f\"Unmapped labels found in 'class': {bad}\")\n",
    "\n",
    "    X = df_fe[feature_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    out = pd.concat([y.astype(int).rename(\"label\"), X], axis=1)\n",
    "    return out\n",
    "\n",
    "train_ready = to_xgb_ready(df_train_fe, feature_cols, LABEL_TO_ID)\n",
    "test_ready  = to_xgb_ready(df_test_fe,  feature_cols, LABEL_TO_ID)\n",
    "prod_ready  = to_xgb_ready(df_prod_fe,  feature_cols, LABEL_TO_ID)\n",
    "\n",
    "job_tag = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "S3_FEATURE_COLS_JSON = f\"{S3_PREP}{job_tag}/feature_cols.json\"\n",
    "S3_TRAIN_XGB = f\"{S3_PREP}{job_tag}/train.csv\"     \n",
    "S3_VAL_XGB   = f\"{S3_PREP}{job_tag}/validation.csv\"  \n",
    "S3_PROD_X    = f\"{S3_PREP}{job_tag}/prod_features.csv\" \n",
    "\n",
    "# Basic split for validation\n",
    "val_frac = 0.2\n",
    "val_n = int(len(train_ready) * val_frac)\n",
    "train_part = train_ready.iloc[val_n:].reset_index(drop=True)\n",
    "val_part   = train_ready.iloc[:val_n].reset_index(drop=True)\n",
    "\n",
    "s3_put_bytes(json.dumps(feature_cols).encode(\"utf-8\"), S3_FEATURE_COLS_JSON, \"application/json\")\n",
    "s3_put_bytes(train_part.to_csv(index=False, header=False).encode(\"utf-8\"), S3_TRAIN_XGB, \"text/csv\")\n",
    "s3_put_bytes(val_part.to_csv(index=False, header=False).encode(\"utf-8\"),   S3_VAL_XGB,   \"text/csv\")\n",
    "s3_put_bytes(prod_ready.drop(columns=[\"label\"]).to_csv(index=False, header=False).encode(\"utf-8\"), S3_PROD_X, \"text/csv\")\n",
    "\n",
    "print(\"Prepared training inputs:\")\n",
    "print(\"  feature_cols:\", S3_FEATURE_COLS_JSON)\n",
    "print(\"  train:\", S3_TRAIN_XGB)\n",
    "print(\"  val:  \", S3_VAL_XGB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1481bb09-1cd2-4172-aaf0-ec9d38db30db",
   "metadata": {},
   "source": [
    "### Model Training & Evaluation (SageMaker built-in XGBoost) and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38a919-e3fa-47ee-b88c-6df22ca3fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker:Creating training-job with name: xgb-train-1-20260217-215414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-17 22:13:46 Starting - Starting the training job...\n",
      "2026-02-17 22:14:02 Starting - Preparing the instances for training...\n",
      "2026-02-17 22:14:25 Downloading - Downloading input data...\n",
      "2026-02-17 22:14:51 Downloading - Downloading the training image...\n",
      "2026-02-17 22:15:42 Training - Training image download completed. Training in progress...\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:15:49.680 ip-10-2-103-81.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:15:49.758 ip-10-2-103-81.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Failed to parse hyperparameter objective value multi:softprob to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] File path /opt/ml/input/data/train of input files\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Making smlinks from folder /opt/ml/input/data/train to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] creating symlink between Path /opt/ml/input/data/train/train.csv and destination /tmp/sagemaker_xgboost_input_data/train.csv-4877175779577416419\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] File path /opt/ml/input/data/validation of input files\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Making smlinks from folder /opt/ml/input/data/validation to folder /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] creating symlink between Path /opt/ml/input/data/validation/validation.csv and destination /tmp/sagemaker_xgboost_input_data/validation.csv3294780743523530521\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] files path: /tmp/sagemaker_xgboost_input_data\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Train matrix has 38400 rows and 21 columns\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Validation matrix has 9599 rows\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:15:50.211 ip-10-2-103-81.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:15:50.211 ip-10-2-103-81.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:15:50.212 ip-10-2-103-81.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:15:50.212 ip-10-2-103-81.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:15:50:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[0]#011train-mlogloss:0.84145#011validation-mlogloss:0.84090\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:15:50.461 ip-10-2-103-81.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:15:50.467 ip-10-2-103-81.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-mlogloss:0.66503#011validation-mlogloss:0.66438\u001b[0m\n",
      "\u001b[34m[2]#011train-mlogloss:0.53653#011validation-mlogloss:0.53601\u001b[0m\n",
      "\u001b[34m[3]#011train-mlogloss:0.43944#011validation-mlogloss:0.43911\u001b[0m\n",
      "\u001b[34m[4]#011train-mlogloss:0.36449#011validation-mlogloss:0.36416\u001b[0m\n",
      "\u001b[34m[5]#011train-mlogloss:0.30646#011validation-mlogloss:0.30609\u001b[0m\n",
      "\u001b[34m[6]#011train-mlogloss:0.26089#011validation-mlogloss:0.26058\u001b[0m\n",
      "\u001b[34m[7]#011train-mlogloss:0.22431#011validation-mlogloss:0.22421\u001b[0m\n",
      "\u001b[34m[8]#011train-mlogloss:0.19511#011validation-mlogloss:0.19505\u001b[0m\n",
      "\u001b[34m[9]#011train-mlogloss:0.17128#011validation-mlogloss:0.17144\u001b[0m\n",
      "\u001b[34m[10]#011train-mlogloss:0.15220#011validation-mlogloss:0.15257\u001b[0m\n",
      "\u001b[34m[11]#011train-mlogloss:0.13704#011validation-mlogloss:0.13782\u001b[0m\n",
      "\u001b[34m[12]#011train-mlogloss:0.12468#011validation-mlogloss:0.12555\u001b[0m\n",
      "\u001b[34m[13]#011train-mlogloss:0.11456#011validation-mlogloss:0.11574\u001b[0m\n",
      "\u001b[34m[14]#011train-mlogloss:0.10600#011validation-mlogloss:0.10750\u001b[0m\n",
      "\u001b[34m[15]#011train-mlogloss:0.09903#011validation-mlogloss:0.10086\u001b[0m\n",
      "\u001b[34m[16]#011train-mlogloss:0.09336#011validation-mlogloss:0.09553\u001b[0m\n",
      "\u001b[34m[17]#011train-mlogloss:0.08848#011validation-mlogloss:0.09115\u001b[0m\n",
      "\u001b[34m[18]#011train-mlogloss:0.08448#011validation-mlogloss:0.08765\u001b[0m\n",
      "\u001b[34m[19]#011train-mlogloss:0.08092#011validation-mlogloss:0.08465\u001b[0m\n",
      "\u001b[34m[20]#011train-mlogloss:0.07814#011validation-mlogloss:0.08228\u001b[0m\n",
      "\u001b[34m[21]#011train-mlogloss:0.07588#011validation-mlogloss:0.08050\u001b[0m\n",
      "\u001b[34m[22]#011train-mlogloss:0.07377#011validation-mlogloss:0.07872\u001b[0m\n",
      "\u001b[34m[23]#011train-mlogloss:0.07217#011validation-mlogloss:0.07756\u001b[0m\n",
      "\u001b[34m[24]#011train-mlogloss:0.07004#011validation-mlogloss:0.07639\u001b[0m\n",
      "\u001b[34m[25]#011train-mlogloss:0.06842#011validation-mlogloss:0.07532\u001b[0m\n",
      "\u001b[34m[26]#011train-mlogloss:0.06715#011validation-mlogloss:0.07456\u001b[0m\n",
      "\u001b[34m[27]#011train-mlogloss:0.06613#011validation-mlogloss:0.07393\u001b[0m\n",
      "\u001b[34m[28]#011train-mlogloss:0.06523#011validation-mlogloss:0.07343\u001b[0m\n",
      "\u001b[34m[29]#011train-mlogloss:0.06421#011validation-mlogloss:0.07297\u001b[0m\n",
      "\u001b[34m[30]#011train-mlogloss:0.06338#011validation-mlogloss:0.07266\u001b[0m\n",
      "\u001b[34m[31]#011train-mlogloss:0.06265#011validation-mlogloss:0.07229\u001b[0m\n",
      "\u001b[34m[32]#011train-mlogloss:0.06111#011validation-mlogloss:0.07162\u001b[0m\n",
      "\u001b[34m[33]#011train-mlogloss:0.06013#011validation-mlogloss:0.07121\u001b[0m\n",
      "\u001b[34m[34]#011train-mlogloss:0.05912#011validation-mlogloss:0.07102\u001b[0m\n",
      "\u001b[34m[35]#011train-mlogloss:0.05829#011validation-mlogloss:0.07066\u001b[0m\n",
      "\u001b[34m[36]#011train-mlogloss:0.05784#011validation-mlogloss:0.07057\u001b[0m\n",
      "\u001b[34m[37]#011train-mlogloss:0.05681#011validation-mlogloss:0.07024\u001b[0m\n",
      "\u001b[34m[38]#011train-mlogloss:0.05618#011validation-mlogloss:0.07001\u001b[0m\n",
      "\u001b[34m[39]#011train-mlogloss:0.05566#011validation-mlogloss:0.06998\u001b[0m\n",
      "\u001b[34m[40]#011train-mlogloss:0.05518#011validation-mlogloss:0.06980\u001b[0m\n",
      "\u001b[34m[41]#011train-mlogloss:0.05454#011validation-mlogloss:0.06971\u001b[0m\n",
      "\u001b[34m[42]#011train-mlogloss:0.05372#011validation-mlogloss:0.06955\u001b[0m\n",
      "\u001b[34m[43]#011train-mlogloss:0.05276#011validation-mlogloss:0.06945\u001b[0m\n",
      "\u001b[34m[44]#011train-mlogloss:0.05232#011validation-mlogloss:0.06939\u001b[0m\n",
      "\u001b[34m[45]#011train-mlogloss:0.05148#011validation-mlogloss:0.06945\u001b[0m\n",
      "\u001b[34m[46]#011train-mlogloss:0.05095#011validation-mlogloss:0.06924\u001b[0m\n",
      "\u001b[34m[47]#011train-mlogloss:0.05038#011validation-mlogloss:0.06928\u001b[0m\n",
      "\u001b[34m[48]#011train-mlogloss:0.05007#011validation-mlogloss:0.06927\u001b[0m\n",
      "\u001b[34m[49]#011train-mlogloss:0.04953#011validation-mlogloss:0.06936\u001b[0m\n",
      "\u001b[34m[50]#011train-mlogloss:0.04908#011validation-mlogloss:0.06932\u001b[0m\n",
      "\u001b[34m[51]#011train-mlogloss:0.04864#011validation-mlogloss:0.06929\u001b[0m\n",
      "\u001b[34m[52]#011train-mlogloss:0.04814#011validation-mlogloss:0.06937\u001b[0m\n",
      "\u001b[34m[53]#011train-mlogloss:0.04785#011validation-mlogloss:0.06942\u001b[0m\n",
      "\u001b[34m[54]#011train-mlogloss:0.04715#011validation-mlogloss:0.06937\u001b[0m\n",
      "\u001b[34m[55]#011train-mlogloss:0.04653#011validation-mlogloss:0.06920\u001b[0m\n",
      "\u001b[34m[56]#011train-mlogloss:0.04613#011validation-mlogloss:0.06916\u001b[0m\n",
      "\u001b[34m[57]#011train-mlogloss:0.04563#011validation-mlogloss:0.06923\u001b[0m\n",
      "\u001b[34m[58]#011train-mlogloss:0.04525#011validation-mlogloss:0.06910\u001b[0m\n",
      "\u001b[34m[59]#011train-mlogloss:0.04482#011validation-mlogloss:0.06907\u001b[0m\n",
      "\u001b[34m[60]#011train-mlogloss:0.04420#011validation-mlogloss:0.06902\u001b[0m\n",
      "\u001b[34m[61]#011train-mlogloss:0.04383#011validation-mlogloss:0.06900\u001b[0m\n",
      "\u001b[34m[62]#011train-mlogloss:0.04326#011validation-mlogloss:0.06903\u001b[0m\n",
      "\u001b[34m[63]#011train-mlogloss:0.04287#011validation-mlogloss:0.06902\u001b[0m\n",
      "\u001b[34m[64]#011train-mlogloss:0.04263#011validation-mlogloss:0.06906\u001b[0m\n",
      "\u001b[34m[65]#011train-mlogloss:0.04214#011validation-mlogloss:0.06914\u001b[0m\n",
      "\u001b[34m[66]#011train-mlogloss:0.04181#011validation-mlogloss:0.06905\u001b[0m\n",
      "\u001b[34m[67]#011train-mlogloss:0.04141#011validation-mlogloss:0.06908\u001b[0m\n",
      "\u001b[34m[68]#011train-mlogloss:0.04099#011validation-mlogloss:0.06909\u001b[0m\n",
      "\u001b[34m[69]#011train-mlogloss:0.04070#011validation-mlogloss:0.06912\u001b[0m\n",
      "\u001b[34m[70]#011train-mlogloss:0.04041#011validation-mlogloss:0.06927\u001b[0m\n",
      "\u001b[34m[71]#011train-mlogloss:0.04003#011validation-mlogloss:0.06929\u001b[0m\n",
      "\u001b[34m[72]#011train-mlogloss:0.03962#011validation-mlogloss:0.06925\u001b[0m\n",
      "\u001b[34m[73]#011train-mlogloss:0.03923#011validation-mlogloss:0.06912\u001b[0m\n",
      "\u001b[34m[74]#011train-mlogloss:0.03881#011validation-mlogloss:0.06898\u001b[0m\n",
      "\u001b[34m[75]#011train-mlogloss:0.03851#011validation-mlogloss:0.06909\u001b[0m\n",
      "\u001b[34m[76]#011train-mlogloss:0.03830#011validation-mlogloss:0.06917\u001b[0m\n",
      "\u001b[34m[77]#011train-mlogloss:0.03801#011validation-mlogloss:0.06933\u001b[0m\n",
      "\u001b[34m[78]#011train-mlogloss:0.03762#011validation-mlogloss:0.06934\u001b[0m\n",
      "\u001b[34m[79]#011train-mlogloss:0.03732#011validation-mlogloss:0.06940\u001b[0m\n",
      "\u001b[34m[80]#011train-mlogloss:0.03708#011validation-mlogloss:0.06942\u001b[0m\n",
      "\u001b[34m[81]#011train-mlogloss:0.03688#011validation-mlogloss:0.06946\u001b[0m\n",
      "\u001b[34m[82]#011train-mlogloss:0.03659#011validation-mlogloss:0.06945\u001b[0m\n",
      "\u001b[34m[83]#011train-mlogloss:0.03622#011validation-mlogloss:0.06962\u001b[0m\n",
      "\u001b[34m[84]#011train-mlogloss:0.03599#011validation-mlogloss:0.06965\u001b[0m\n",
      "\u001b[34m[85]#011train-mlogloss:0.03567#011validation-mlogloss:0.06970\u001b[0m\n",
      "\u001b[34m[86]#011train-mlogloss:0.03526#011validation-mlogloss:0.06970\u001b[0m\n",
      "\u001b[34m[87]#011train-mlogloss:0.03487#011validation-mlogloss:0.06987\u001b[0m\n",
      "\u001b[34m[88]#011train-mlogloss:0.03443#011validation-mlogloss:0.06986\u001b[0m\n",
      "\u001b[34m[89]#011train-mlogloss:0.03416#011validation-mlogloss:0.06997\u001b[0m\n",
      "\u001b[34m[90]#011train-mlogloss:0.03384#011validation-mlogloss:0.07009\u001b[0m\n",
      "\u001b[34m[91]#011train-mlogloss:0.03349#011validation-mlogloss:0.07002\u001b[0m\n",
      "\u001b[34m[92]#011train-mlogloss:0.03310#011validation-mlogloss:0.06993\u001b[0m\n",
      "\u001b[34m[93]#011train-mlogloss:0.03276#011validation-mlogloss:0.06991\u001b[0m\n",
      "\u001b[34m[94]#011train-mlogloss:0.03248#011validation-mlogloss:0.06986\u001b[0m\n",
      "\u001b[34m[95]#011train-mlogloss:0.03223#011validation-mlogloss:0.06983\u001b[0m\n",
      "\u001b[34m[96]#011train-mlogloss:0.03187#011validation-mlogloss:0.06975\u001b[0m\n",
      "\u001b[34m[97]#011train-mlogloss:0.03157#011validation-mlogloss:0.06969\u001b[0m\n",
      "\u001b[34m[98]#011train-mlogloss:0.03131#011validation-mlogloss:0.06967\u001b[0m\n",
      "\u001b[34m[99]#011train-mlogloss:0.03105#011validation-mlogloss:0.06970\u001b[0m\n",
      "\u001b[34m[100]#011train-mlogloss:0.03077#011validation-mlogloss:0.06974\u001b[0m\n",
      "\u001b[34m[101]#011train-mlogloss:0.03055#011validation-mlogloss:0.06975\u001b[0m\n",
      "\u001b[34m[102]#011train-mlogloss:0.03035#011validation-mlogloss:0.06980\u001b[0m\n",
      "\u001b[34m[103]#011train-mlogloss:0.03001#011validation-mlogloss:0.06989\u001b[0m\n",
      "\u001b[34m[104]#011train-mlogloss:0.02967#011validation-mlogloss:0.07002\u001b[0m\n",
      "\u001b[34m[105]#011train-mlogloss:0.02941#011validation-mlogloss:0.07003\u001b[0m\n",
      "\u001b[34m[106]#011train-mlogloss:0.02913#011validation-mlogloss:0.07004\u001b[0m\n",
      "\u001b[34m[107]#011train-mlogloss:0.02879#011validation-mlogloss:0.07009\u001b[0m\n",
      "\u001b[34m[108]#011train-mlogloss:0.02857#011validation-mlogloss:0.07005\u001b[0m\n",
      "\u001b[34m[109]#011train-mlogloss:0.02827#011validation-mlogloss:0.06996\u001b[0m\n",
      "\u001b[34m[110]#011train-mlogloss:0.02799#011validation-mlogloss:0.07008\u001b[0m\n",
      "\u001b[34m[111]#011train-mlogloss:0.02772#011validation-mlogloss:0.07004\u001b[0m\n",
      "\u001b[34m[112]#011train-mlogloss:0.02751#011validation-mlogloss:0.07012\u001b[0m\n",
      "\u001b[34m[113]#011train-mlogloss:0.02724#011validation-mlogloss:0.07019\u001b[0m\n",
      "\u001b[34m[114]#011train-mlogloss:0.02702#011validation-mlogloss:0.07029\u001b[0m\n",
      "\u001b[34m[115]#011train-mlogloss:0.02684#011validation-mlogloss:0.07033\u001b[0m\n",
      "\u001b[34m[116]#011train-mlogloss:0.02669#011validation-mlogloss:0.07037\u001b[0m\n",
      "\u001b[34m[117]#011train-mlogloss:0.02645#011validation-mlogloss:0.07045\u001b[0m\n",
      "\u001b[34m[118]#011train-mlogloss:0.02627#011validation-mlogloss:0.07059\u001b[0m\n",
      "\u001b[34m[119]#011train-mlogloss:0.02613#011validation-mlogloss:0.07069\u001b[0m\n",
      "\u001b[34m[120]#011train-mlogloss:0.02592#011validation-mlogloss:0.07074\u001b[0m\n",
      "\u001b[34m[121]#011train-mlogloss:0.02566#011validation-mlogloss:0.07087\u001b[0m\n",
      "\u001b[34m[122]#011train-mlogloss:0.02551#011validation-mlogloss:0.07089\u001b[0m\n",
      "\u001b[34m[123]#011train-mlogloss:0.02533#011validation-mlogloss:0.07094\u001b[0m\n",
      "\u001b[34m[124]#011train-mlogloss:0.02506#011validation-mlogloss:0.07103\u001b[0m\n",
      "\u001b[34m[125]#011train-mlogloss:0.02478#011validation-mlogloss:0.07109\u001b[0m\n",
      "\u001b[34m[126]#011train-mlogloss:0.02452#011validation-mlogloss:0.07114\u001b[0m\n",
      "\u001b[34m[127]#011train-mlogloss:0.02436#011validation-mlogloss:0.07126\u001b[0m\n",
      "\u001b[34m[128]#011train-mlogloss:0.02411#011validation-mlogloss:0.07135\u001b[0m\n",
      "\u001b[34m[129]#011train-mlogloss:0.02387#011validation-mlogloss:0.07142\u001b[0m\n",
      "\u001b[34m[130]#011train-mlogloss:0.02378#011validation-mlogloss:0.07150\u001b[0m\n",
      "\u001b[34m[131]#011train-mlogloss:0.02367#011validation-mlogloss:0.07153\u001b[0m\n",
      "\u001b[34m[132]#011train-mlogloss:0.02353#011validation-mlogloss:0.07154\u001b[0m\n",
      "\u001b[34m[133]#011train-mlogloss:0.02335#011validation-mlogloss:0.07167\u001b[0m\n",
      "\u001b[34m[134]#011train-mlogloss:0.02328#011validation-mlogloss:0.07166\u001b[0m\n",
      "\u001b[34m[135]#011train-mlogloss:0.02305#011validation-mlogloss:0.07166\u001b[0m\n",
      "\u001b[34m[136]#011train-mlogloss:0.02292#011validation-mlogloss:0.07173\u001b[0m\n",
      "\u001b[34m[137]#011train-mlogloss:0.02274#011validation-mlogloss:0.07176\u001b[0m\n",
      "\u001b[34m[138]#011train-mlogloss:0.02258#011validation-mlogloss:0.07185\u001b[0m\n",
      "\u001b[34m[139]#011train-mlogloss:0.02237#011validation-mlogloss:0.07185\u001b[0m\n",
      "\u001b[34m[140]#011train-mlogloss:0.02210#011validation-mlogloss:0.07191\u001b[0m\n",
      "\u001b[34m[141]#011train-mlogloss:0.02199#011validation-mlogloss:0.07189\u001b[0m\n",
      "\u001b[34m[142]#011train-mlogloss:0.02179#011validation-mlogloss:0.07192\u001b[0m\n",
      "\u001b[34m[143]#011train-mlogloss:0.02155#011validation-mlogloss:0.07197\u001b[0m\n",
      "\u001b[34m[144]#011train-mlogloss:0.02135#011validation-mlogloss:0.07212\u001b[0m\n",
      "\u001b[34m[145]#011train-mlogloss:0.02121#011validation-mlogloss:0.07221\u001b[0m\n",
      "\u001b[34m[146]#011train-mlogloss:0.02098#011validation-mlogloss:0.07214\u001b[0m\n",
      "\u001b[34m[147]#011train-mlogloss:0.02086#011validation-mlogloss:0.07221\u001b[0m\n",
      "\u001b[34m[148]#011train-mlogloss:0.02072#011validation-mlogloss:0.07217\u001b[0m\n",
      "\u001b[34m[149]#011train-mlogloss:0.02060#011validation-mlogloss:0.07219\u001b[0m\n",
      "\u001b[34m[150]#011train-mlogloss:0.02046#011validation-mlogloss:0.07227\u001b[0m\n",
      "\u001b[34m[151]#011train-mlogloss:0.02033#011validation-mlogloss:0.07231\u001b[0m\n",
      "\u001b[34m[152]#011train-mlogloss:0.02025#011validation-mlogloss:0.07242\u001b[0m\n",
      "\u001b[34m[153]#011train-mlogloss:0.02008#011validation-mlogloss:0.07251\u001b[0m\n",
      "\u001b[34m[154]#011train-mlogloss:0.01998#011validation-mlogloss:0.07254\u001b[0m\n",
      "\u001b[34m[155]#011train-mlogloss:0.01980#011validation-mlogloss:0.07270\u001b[0m\n",
      "\u001b[34m[156]#011train-mlogloss:0.01965#011validation-mlogloss:0.07257\u001b[0m\n",
      "\u001b[34m[157]#011train-mlogloss:0.01957#011validation-mlogloss:0.07253\u001b[0m\n",
      "\u001b[34m[158]#011train-mlogloss:0.01941#011validation-mlogloss:0.07267\u001b[0m\n",
      "\u001b[34m[159]#011train-mlogloss:0.01920#011validation-mlogloss:0.07263\u001b[0m\n",
      "\u001b[34m[160]#011train-mlogloss:0.01910#011validation-mlogloss:0.07264\u001b[0m\n",
      "\u001b[34m[161]#011train-mlogloss:0.01896#011validation-mlogloss:0.07267\u001b[0m\n",
      "\u001b[34m[162]#011train-mlogloss:0.01886#011validation-mlogloss:0.07272\u001b[0m\n",
      "\u001b[34m[163]#011train-mlogloss:0.01874#011validation-mlogloss:0.07277\u001b[0m\n",
      "\u001b[34m[164]#011train-mlogloss:0.01865#011validation-mlogloss:0.07278\u001b[0m\n",
      "\u001b[34m[165]#011train-mlogloss:0.01846#011validation-mlogloss:0.07277\u001b[0m\n",
      "\u001b[34m[166]#011train-mlogloss:0.01831#011validation-mlogloss:0.07288\u001b[0m\n",
      "\u001b[34m[167]#011train-mlogloss:0.01818#011validation-mlogloss:0.07296\u001b[0m\n",
      "\u001b[34m[168]#011train-mlogloss:0.01804#011validation-mlogloss:0.07293\u001b[0m\n",
      "\u001b[34m[169]#011train-mlogloss:0.01791#011validation-mlogloss:0.07300\u001b[0m\n",
      "\u001b[34m[170]#011train-mlogloss:0.01774#011validation-mlogloss:0.07304\u001b[0m\n",
      "\u001b[34m[171]#011train-mlogloss:0.01756#011validation-mlogloss:0.07310\u001b[0m\n",
      "\u001b[34m[172]#011train-mlogloss:0.01740#011validation-mlogloss:0.07313\u001b[0m\n",
      "\u001b[34m[173]#011train-mlogloss:0.01730#011validation-mlogloss:0.07331\u001b[0m\n",
      "\u001b[34m[174]#011train-mlogloss:0.01720#011validation-mlogloss:0.07337\u001b[0m\n",
      "\u001b[34m[175]#011train-mlogloss:0.01713#011validation-mlogloss:0.07346\u001b[0m\n",
      "\u001b[34m[176]#011train-mlogloss:0.01703#011validation-mlogloss:0.07351\u001b[0m\n",
      "\u001b[34m[177]#011train-mlogloss:0.01693#011validation-mlogloss:0.07352\u001b[0m\n",
      "\u001b[34m[178]#011train-mlogloss:0.01676#011validation-mlogloss:0.07355\u001b[0m\n",
      "\u001b[34m[179]#011train-mlogloss:0.01665#011validation-mlogloss:0.07367\u001b[0m\n",
      "\u001b[34m[180]#011train-mlogloss:0.01653#011validation-mlogloss:0.07380\u001b[0m\n",
      "\u001b[34m[181]#011train-mlogloss:0.01636#011validation-mlogloss:0.07393\u001b[0m\n",
      "\u001b[34m[182]#011train-mlogloss:0.01621#011validation-mlogloss:0.07389\u001b[0m\n",
      "\u001b[34m[183]#011train-mlogloss:0.01612#011validation-mlogloss:0.07393\u001b[0m\n",
      "\u001b[34m[184]#011train-mlogloss:0.01601#011validation-mlogloss:0.07393\u001b[0m\n",
      "\u001b[34m[185]#011train-mlogloss:0.01587#011validation-mlogloss:0.07385\u001b[0m\n",
      "\u001b[34m[186]#011train-mlogloss:0.01575#011validation-mlogloss:0.07392\u001b[0m\n",
      "\u001b[34m[187]#011train-mlogloss:0.01563#011validation-mlogloss:0.07403\u001b[0m\n",
      "\u001b[34m[188]#011train-mlogloss:0.01553#011validation-mlogloss:0.07404\u001b[0m\n",
      "\u001b[34m[189]#011train-mlogloss:0.01540#011validation-mlogloss:0.07412\u001b[0m\n",
      "\u001b[34m[190]#011train-mlogloss:0.01527#011validation-mlogloss:0.07411\u001b[0m\n",
      "\u001b[34m[191]#011train-mlogloss:0.01514#011validation-mlogloss:0.07425\u001b[0m\n",
      "\u001b[34m[192]#011train-mlogloss:0.01502#011validation-mlogloss:0.07426\u001b[0m\n",
      "\u001b[34m[193]#011train-mlogloss:0.01487#011validation-mlogloss:0.07439\u001b[0m\n",
      "\u001b[34m[194]#011train-mlogloss:0.01478#011validation-mlogloss:0.07441\u001b[0m\n",
      "\u001b[34m[195]#011train-mlogloss:0.01464#011validation-mlogloss:0.07446\u001b[0m\n",
      "\u001b[34m[196]#011train-mlogloss:0.01453#011validation-mlogloss:0.07448\u001b[0m\n",
      "\u001b[34m[197]#011train-mlogloss:0.01442#011validation-mlogloss:0.07454\u001b[0m\n",
      "\u001b[34m[198]#011train-mlogloss:0.01433#011validation-mlogloss:0.07459\u001b[0m\n",
      "\u001b[34m[199]#011train-mlogloss:0.01420#011validation-mlogloss:0.07467\u001b[0m\n",
      "\n",
      "2026-02-17 22:16:40 Uploading - Uploading generated training model\n",
      "2026-02-17 22:16:40 Completed - Training job completed\n",
      "Training seconds: 135\n",
      "Billable seconds: 135\n",
      "Training job: xgb-train-1-20260217-215414\n",
      "Model artifacts: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/output/xgb-train-1-20260217-215414/xgb-train-1-20260217-215414/output/model.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2026-02-17-22-16-59-894\n",
      "INFO:sagemaker:Creating transform job with name: sagemaker-xgboost-2026-02-17-22-17-00-667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:56:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [18] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:56:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [18] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [18] [INFO] Listening at: unix:/tmp/gunicorn.sock (18)\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [18] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [18] [INFO] Listening at: unix:/tmp/gunicorn.sock (18)\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [18] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:22:02:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:22:02:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:22:02:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:22:02:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:22:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"POST /invocations HTTP/1.1\" 200 803919 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:22:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"POST /invocations HTTP/1.1\" 200 803919 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-02-17T22:22:02.126:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:56:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [18] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:56:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:56:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [18] [INFO] Starting gunicorn 23.0.0\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [18] [INFO] Listening at: unix:/tmp/gunicorn.sock (18)\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [18] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[34m[2026-02-17 22:21:56 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:58:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:21:59:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [18] [INFO] Listening at: unix:/tmp/gunicorn.sock (18)\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [18] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[35m[2026-02-17 22:21:56 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:58:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:21:59:INFO] Model objective : multi:softprob\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:22:02:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:22:02:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:22:02:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:22:02:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2026-02-17:22:22:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"POST /invocations HTTP/1.1\" 200 803919 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2026-02-17:22:22:02:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.9/site-packages/xgboost/core.py:122: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [17/Feb/2026:22:22:02 +0000] \"POST /invocations HTTP/1.1\" 200 803919 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2026-02-17T22:22:02.126:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Batch output prefix: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/output/xgb-batch-20260217-215414/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except Exception:\n",
    "    role = boto3.client(\"iam\").get_role(RoleName=\"LabRole\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "image = sagemaker.image_uris.retrieve(\"xgboost\", region=region, version=\"1.7-1\")\n",
    "\n",
    "train_job_name = f\"xgb-train-1-{job_tag}\"\n",
    "output_path = f\"{S3_OUT}{train_job_name}/\"\n",
    "\n",
    "est = Estimator(\n",
    "    image_uri=image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size=50,\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_path,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "est.set_hyperparameters(\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=3,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    subsample=0.8,\n",
    "    num_round=200,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "est.fit(\n",
    "    {\n",
    "        \"train\": TrainingInput(S3_TRAIN_XGB, content_type=\"text/csv\"),\n",
    "        \"validation\": TrainingInput(S3_VAL_XGB, content_type=\"text/csv\"),\n",
    "    },\n",
    "    job_name=train_job_name,\n",
    "    logs=True,\n",
    ")\n",
    "\n",
    "print(\"Training job:\", train_job_name)\n",
    "print(\"Model artifacts:\", est.model_data)\n",
    "\n",
    "\n",
    "S3_TEST_X = f\"{S3_PREP}{job_tag}/test_features.csv\"\n",
    "s3_put_bytes(test_ready.drop(columns=[\"label\"]).to_csv(index=False, header=False).encode(\"utf-8\"), S3_TEST_X, \"text/csv\")\n",
    "\n",
    "bt_name = f\"xgb-batch-{job_tag}\"\n",
    "bt_out  = f\"{S3_OUT}{bt_name}/\"\n",
    "\n",
    "transformer = est.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    output_path=bt_out,\n",
    "    assemble_with=\"Line\",\n",
    "    accept=\"text/csv\",\n",
    ")\n",
    "\n",
    "transformer.transform(data=S3_TEST_X, content_type=\"text/csv\", split_type=\"Line\")\n",
    "transformer.wait()\n",
    "print(\"Batch output prefix:\", bt_out)\n",
    "\n",
    "def read_first_output_file_text(s3_prefix_uri: str) -> str:\n",
    "    b, pfx, objs = list_s3_objects(s3_prefix_uri)\n",
    "    keys = [o[\"Key\"] for o in objs if not o[\"Key\"].endswith(\"_SUCCESS\")]\n",
    "    out_keys = [k for k in keys if k.endswith(\".out\")] or keys\n",
    "    if not out_keys:\n",
    "        raise RuntimeError(f\"No batch output files found under: {s3_prefix_uri}\")\n",
    "    out_key = sorted(out_keys)[0]\n",
    "    body = s3.get_object(Bucket=b, Key=out_key)[\"Body\"].read().decode(\"utf-8\", errors=\"replace\")\n",
    "    return body\n",
    "\n",
    "# parsing probabilities from batch output (one line per record)\n",
    "out_text = read_first_output_file_text(bt_out)\n",
    "\n",
    "pred_probs = []\n",
    "for ln in out_text.splitlines():\n",
    "    ln = ln.strip()\n",
    "    if not ln:\n",
    "        continue\n",
    "\n",
    "    if ln.startswith(\"[\") and ln.endswith(\"]\"):\n",
    "        probs = json.loads(ln)\n",
    "        pred_probs.append([float(x) for x in probs])\n",
    "        continue\n",
    "\n",
    "    if re.fullmatch(r\"[-+0-9.eE]+(,[-+0-9.eE]+)+\", ln):\n",
    "        pred_probs.append([float(x) for x in ln.split(\",\")])\n",
    "        continue\n",
    "\n",
    "    raise ValueError(f\"Unrecognized prediction line format: {ln[:200]}\")\n",
    "\n",
    "pred_probs = np.asarray(pred_probs, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d00c9630-1ef7-48ba-a8fc-2396b0f3091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2026-02-17-22-23-10-552\n",
      "INFO:sagemaker:Creating endpoint-config with name xgb-ep1-20260217-215414\n",
      "INFO:sagemaker:Creating endpoint with name xgb-ep1-20260217-215414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Endpoint: xgb-ep1-20260217-215414\n",
      "Data capture S3: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/monitoring/datacapture/\n"
     ]
    }
   ],
   "source": [
    "data_capture_prefix = f\"{S3_MON}datacapture/\"\n",
    "\n",
    "data_capture = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=data_capture_prefix,\n",
    "    capture_options=[\"Input\", \"Output\"],\n",
    ")\n",
    "\n",
    "endpoint_name = f\"xgb-ep1-{job_tag}\"\n",
    "\n",
    "predictor = est.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture,\n",
    ")\n",
    "\n",
    "predictor.serializer = CSVSerializer()\n",
    "predictor.deserializer = StringDeserializer()\n",
    "\n",
    "print(\"Endpoint:\", endpoint_name)\n",
    "print(\"Data capture S3:\", data_capture_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13176063-fcaf-43fe-9da5-26da2d744e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint TEST metrics:\n",
      "macro_f1: 0.975357\n",
      "balanced_acc: 0.971641\n"
     ]
    }
   ],
   "source": [
    "X_test = (\n",
    "    df_test_fe[feature_cols]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .fillna(0.0)\n",
    "    .to_numpy(dtype=float)\n",
    ")\n",
    "\n",
    "y_true = (\n",
    "    df_test_fe[\"class\"]\n",
    "    .astype(str).str.strip().str.upper()\n",
    "    .map(LABEL_TO_ID)\n",
    "    .to_numpy(dtype=int)\n",
    ")\n",
    "\n",
    "def parse_probs_from_response(resp_text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns array shape (n_rows, n_classes).\n",
    "    Supports:\n",
    "      1) {\"predictions\":[{\"score\":[...]}, ...]}\n",
    "      2) {\"predictions\":[[...],[...],...]}\n",
    "      3) line-based outputs: \"p0,p1,p2\" or \"[p0,p1,p2]\"\n",
    "    \"\"\"\n",
    "    s = str(resp_text).strip()\n",
    "    if s.startswith(\"{\") or s.startswith(\"[\"):\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict) and \"predictions\" in obj:\n",
    "            preds = obj[\"predictions\"]\n",
    "            if len(preds) == 0:\n",
    "                return np.zeros((0, 0), dtype=float)\n",
    "            if isinstance(preds[0], dict) and \"score\" in preds[0]:\n",
    "                return np.asarray([p[\"score\"] for p in preds], dtype=float)\n",
    "            if isinstance(preds[0], list):\n",
    "                return np.asarray(preds, dtype=float)\n",
    "\n",
    "            raise ValueError(f\"Unknown predictions element type: {type(preds[0])}\")\n",
    "        if isinstance(obj, list) and len(obj) > 0 and isinstance(obj[0], list):\n",
    "            return np.asarray(obj, dtype=float)\n",
    "\n",
    "    probs = []\n",
    "    for ln in s.splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln:\n",
    "            continue\n",
    "        if ln.startswith(\"[\") and ln.endswith(\"]\"):\n",
    "            probs.append(json.loads(ln))\n",
    "        else:\n",
    "            probs.append([float(x) for x in ln.split(\",\")])\n",
    "    return np.asarray(probs, dtype=float)\n",
    "\n",
    "#limiting batch size to keep payload redable and avoid timeouts\n",
    "batch_size = 500 \n",
    "all_probs = []\n",
    "\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    chunk = X_test[i:i + batch_size]\n",
    "    payload = \"\\n\".join(\",\".join(map(str, row)) for row in chunk)\n",
    "\n",
    "    resp = predictor.predict(payload)\n",
    "    probs = parse_probs_from_response(resp)\n",
    "\n",
    "    if probs.shape[0] != len(chunk):\n",
    "        raise ValueError(f\"Row count mismatch: sent {len(chunk)} rows, got {probs.shape[0]} predictions\")\n",
    "\n",
    "    all_probs.append(probs)\n",
    "\n",
    "pred_probs = np.vstack(all_probs)\n",
    "y_pred = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "bal = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"Endpoint TEST metrics:\")\n",
    "print(\"macro_f1:\", round(float(macro), 6))\n",
    "print(\"balanced_acc:\", round(float(bal), 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0596f64-538f-4407-a994-98dcf9aab6b3",
   "metadata": {},
   "source": [
    "### Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29090edc-bc52-4bd8-8962-6abc1d7d9f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2026-02-17-22-28-09-287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................\u001b[34m2026-02-17 22:30:55.207886: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:55.207925: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:56.728351: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:56.728389: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:56.728412: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-2-249-196.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:56.728683: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,057 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:579568333234:processing-job/baseline-suggestion-job-2026-02-17-22-28-09-287', 'ProcessingJobName': 'baseline-suggestion-job-2026-02-17-22-28-09-287', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/prepared/20260217-215414/train_fe_header.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/monitoring/baselines/data-quality/', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 50, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::579568333234:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,057 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,057 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,057 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,057 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,057 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,118 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,119 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,119 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,129 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,129 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,129 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,619 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.249.196\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sha\u001b[0m\n",
      "\u001b[34mredcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_462\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,630 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:58,634 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-f4897571-5f9f-47b4-9b00-71c642dd9245\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,141 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,153 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,154 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,156 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,161 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,161 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,161 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,161 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,193 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,205 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,205 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,209 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,212 INFO blockmanagement.BlockManager: The block deletion will start around 2026 Feb 17 22:30:59\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,214 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,214 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,215 INFO util.GSet: 2.0% max memory 3.1 GB = 62.5 MB\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,215 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,252 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,256 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,256 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,256 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,256 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,256 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,256 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,256 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,256 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,256 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,257 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,257 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,283 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,283 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,283 INFO util.GSet: 1.0% max memory 3.1 GB = 31.2 MB\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,283 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,286 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,286 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,286 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,286 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,319 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,323 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,323 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,324 INFO util.GSet: 0.25% max memory 3.1 GB = 7.8 MB\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,324 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,331 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,331 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,331 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,334 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,334 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,336 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,336 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,336 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 959.7 KB\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,336 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,356 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1071432507-10.2.249.196-1771367459351\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,371 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,380 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,461 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,475 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,479 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.249.196\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-02-17 22:30:59,492 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:01,564 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:01,565 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:03,653 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:03,654 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:05,753 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:05,753 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:07,869 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:07,870 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:10,102 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:10,103 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:20,110 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:21,852 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:22,345 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:22,385 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:22,399 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:22,992 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,017 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,017 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,018 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,018 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,042 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11393, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,056 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,058 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,114 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,115 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,115 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,116 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,116 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,467 INFO util.Utils: Successfully started service 'sparkDriver' on port 44183.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,506 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,547 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,566 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,566 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,601 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,624 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-1d321eb9-a8d2-4106-a17c-7fa705f3389c\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,641 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,682 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:23,716 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.249.196:44183/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1771367482987\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:24,195 INFO client.RMProxy: Connecting to ResourceManager at /10.2.249.196:8032\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:24,924 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:24,925 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:24,932 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15536 MB per container)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:24,932 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:24,933 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:24,933 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:24,939 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:25,021 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:27,224 INFO yarn.Client: Uploading resource file:/tmp/spark-ec093cab-be47-46d8-ace3-8804cbe9c093/__spark_libs__7656204672874751892.zip -> hdfs://10.2.249.196/user/root/.sparkStaging/application_1771367465243_0001/__spark_libs__7656204672874751892.zip\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:28,436 INFO yarn.Client: Uploading resource file:/tmp/spark-ec093cab-be47-46d8-ace3-8804cbe9c093/__spark_conf__3312995033133436172.zip -> hdfs://10.2.249.196/user/root/.sparkStaging/application_1771367465243_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:28,480 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:28,480 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:28,481 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:28,481 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:28,481 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:28,511 INFO yarn.Client: Submitting application application_1771367465243_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:28,711 INFO impl.YarnClientImpl: Submitted application application_1771367465243_0001\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:29,716 INFO yarn.Client: Application report for application_1771367465243_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:29,719 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Tue Feb 17 22:31:29 +0000 2026] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1771367488614\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1771367465243_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:30,722 INFO yarn.Client: Application report for application_1771367465243_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:31,725 INFO yarn.Client: Application report for application_1771367465243_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:32,728 INFO yarn.Client: Application report for application_1771367465243_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:33,733 INFO yarn.Client: Application report for application_1771367465243_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,317 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1771367465243_0001), /proxy/application_1771367465243_0001\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,736 INFO yarn.Client: Application report for application_1771367465243_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,737 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.249.196\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1771367488614\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1771367465243_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,739 INFO cluster.YarnClientSchedulerBackend: Application application_1771367465243_0001 has started running.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,757 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37057.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,757 INFO netty.NettyBlockTransferService: Server created on 10.2.249.196:37057\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,760 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,770 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.249.196, 37057, None)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,774 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.249.196:37057 with 1458.6 MiB RAM, BlockManagerId(driver, 10.2.249.196, 37057, None)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,777 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.249.196, 37057, None)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,778 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.249.196, 37057, None)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:34,945 INFO util.log: Logging initialized @14639ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:35,872 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:39,686 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.249.196:44990) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:39,873 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:38941 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 38941, None)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:54,133 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:54,373 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:54,437 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:54,441 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:55,487 INFO datasources.InMemoryFileIndex: It took 37 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:55,660 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:55,988 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:55,992 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.249.196:37057 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:55,998 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,384 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,386 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,391 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,443 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,464 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,464 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,465 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,466 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,474 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,517 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,526 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,527 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.249.196:37057 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,527 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,544 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,545 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,593 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4628 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:56,862 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:38941 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:57,765 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:38941 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:58,096 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1517 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:58,098 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:58,105 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.595 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:58,109 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:58,110 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:58,112 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.668703 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:58,292 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.249.196:37057 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:31:58,306 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:38941 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,628 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,629 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,632 INFO datasources.FileSourceStrategy: Output Data Schema: struct<class: string, alpha: string, delta: string, u: string, g: string ... 20 more fields>\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,848 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,866 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,867 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.249.196:37057 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,869 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,887 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 6617109 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,933 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,935 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,936 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,936 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,939 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:00,942 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:01,038 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:01,040 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:01,041 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.249.196:37057 (size: 8.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:01,041 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:01,042 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:01,042 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:01,046 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:01,085 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:38941 (size: 8.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:01,881 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:38941 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:03,459 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:38941 (size: 17.7 MiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:03,571 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2528 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:03,572 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.627 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:03,573 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:03,573 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:03,573 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:03,573 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.640243 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:03,857 INFO codegen.CodeGenerator: Code generated in 211.593584 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,365 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,507 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,512 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,512 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,513 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,515 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,517 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,538 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 116.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,540 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,541 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.249.196:37057 (size: 35.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,542 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,544 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,544 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,553 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:04,597 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:38941 (size: 35.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,349 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1799 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,350 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,352 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.832 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,353 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,354 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,354 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,355 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,438 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,440 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,440 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,441 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,441 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,442 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,454 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 168.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,456 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,457 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.249.196:37057 (size: 46.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,457 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,458 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,458 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,461 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,480 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:38941 (size: 46.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,523 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,903 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 443 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,903 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,904 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.456 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,904 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,904 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,906 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.467794 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:06,999 INFO codegen.CodeGenerator: Code generated in 75.3244 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,279 INFO codegen.CodeGenerator: Code generated in 33.600845 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,363 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,365 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,365 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,366 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,367 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,367 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,399 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 39.8 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,401 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,402 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.249.196:37057 (size: 17.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,403 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,404 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,404 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,407 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:07,429 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:38941 (size: 17.1 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:09,981 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 2575 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:09,983 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 2.613 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:09,983 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:09,984 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:09,984 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:09,985 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 2.621442 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,454 INFO codegen.CodeGenerator: Code generated in 93.415407 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,462 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,463 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,463 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,464 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,464 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,466 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,471 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 65.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,474 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 21.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,475 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.249.196:37057 (size: 21.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,476 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,476 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,485 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,486 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,502 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:38941 (size: 21.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,789 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 303 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,790 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,790 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.323 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,791 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,791 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,791 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,792 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,871 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.249.196:37057 in memory (size: 35.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,872 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:38941 in memory (size: 35.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,911 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.249.196:37057 in memory (size: 46.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,915 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:38941 in memory (size: 46.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,971 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.249.196:37057 in memory (size: 21.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:10,978 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:38941 in memory (size: 21.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,007 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:38941 in memory (size: 17.1 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,025 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.2.249.196:37057 in memory (size: 17.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,053 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:38941 in memory (size: 8.6 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,054 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.249.196:37057 in memory (size: 8.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,057 INFO codegen.CodeGenerator: Code generated in 111.109059 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,068 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,069 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,069 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,069 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,070 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,071 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,074 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 55.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,075 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,076 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.249.196:37057 (size: 16.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,076 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,077 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,077 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,078 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,093 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:38941 (size: 16.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,099 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,172 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 94 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,172 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,174 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.102 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,174 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,174 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,176 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.107781 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,238 INFO codegen.CodeGenerator: Code generated in 40.974542 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,367 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,371 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,372 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,372 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,373 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,373 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,377 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,385 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,388 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,388 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.249.196:37057 (size: 14.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,389 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,390 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,390 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,392 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,409 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:38941 (size: 14.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,741 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 349 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,741 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,743 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 0.365 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,744 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,744 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,744 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,744 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,745 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,746 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,748 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,749 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.2.249.196:37057 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,750 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,751 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,751 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,754 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,767 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:38941 (size: 3.0 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,779 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,805 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 53 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,805 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,806 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.061 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,806 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,806 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:11,807 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 0.439188 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,032 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,033 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,033 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,033 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,036 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,038 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,047 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 84.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,049 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,050 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.2.249.196:37057 (size: 27.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,051 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,051 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,052 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,053 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,067 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:38941 (size: 27.6 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,549 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 496 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,549 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,550 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.511 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,551 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,551 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,551 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,551 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,587 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,589 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,589 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,589 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,589 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,590 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,597 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 169.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,600 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 46.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,600 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.2.249.196:37057 (size: 46.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,601 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,602 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,602 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,603 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,621 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:38941 (size: 46.5 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,631 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,716 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 113 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,716 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,717 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.126 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,717 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,717 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,718 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.130051 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,851 INFO codegen.CodeGenerator: Code generated in 17.228405 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,886 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,887 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,887 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,887 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,889 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,891 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,897 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 39.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,899 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,900 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.2.249.196:37057 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,900 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,901 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,901 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,903 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:12,914 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:38941 (size: 16.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:13,870 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 968 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:13,871 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:13,871 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.979 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:13,871 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:13,871 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:13,872 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.985911 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,118 INFO codegen.CodeGenerator: Code generated in 59.364959 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,126 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,126 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,126 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,126 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,126 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,127 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,130 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 75.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,133 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,133 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.2.249.196:37057 (size: 23.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,134 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,134 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,135 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,136 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,151 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:38941 (size: 23.9 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,379 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 243 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,379 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,380 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.252 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,380 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,380 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,380 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,380 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,484 INFO codegen.CodeGenerator: Code generated in 55.934518 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,496 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,497 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,497 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,498 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,498 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,499 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,502 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 66.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,505 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,506 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.2.249.196:37057 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,506 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,507 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,507 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,510 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,531 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:38941 (size: 19.2 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,537 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,653 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 143 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,653 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,654 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.154 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,655 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,656 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,656 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.160446 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:14,791 INFO codegen.CodeGenerator: Code generated in 105.584247 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,005 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:38941 in memory (size: 3.0 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,019 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.2.249.196:37057 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,091 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:38941 in memory (size: 16.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,093 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.2.249.196:37057 in memory (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,115 INFO scheduler.DAGScheduler: Registering RDD 77 (collect at AnalysisRunner.scala:326) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,115 INFO scheduler.DAGScheduler: Got map stage job 13 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,115 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,115 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,116 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,117 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[77] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,121 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 84.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,123 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,124 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.2.249.196:37057 (size: 27.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,124 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,125 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[77] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,125 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,126 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,128 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:38941 in memory (size: 16.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,130 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.2.249.196:37057 in memory (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,139 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:38941 (size: 27.6 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,141 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.2.249.196:37057 in memory (size: 46.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,148 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:38941 in memory (size: 46.5 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,198 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.2.249.196:37057 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,203 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:38941 in memory (size: 19.2 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,229 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.2.249.196:37057 in memory (size: 23.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,233 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:38941 in memory (size: 23.9 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,298 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.2.249.196:37057 in memory (size: 27.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,304 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:38941 in memory (size: 27.6 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,344 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.2.249.196:37057 in memory (size: 14.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,351 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:38941 in memory (size: 14.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,559 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 433 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,559 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,560 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (collect at AnalysisRunner.scala:326) finished in 0.442 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,561 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,561 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,561 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,561 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,593 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,594 INFO scheduler.DAGScheduler: Got job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,594 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,595 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,595 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,595 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[80] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,602 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 169.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,604 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 46.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,604 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.2.249.196:37057 (size: 46.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,605 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,605 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[80] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,606 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,607 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,619 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:38941 (size: 46.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,632 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,739 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 15) in 132 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,739 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,742 INFO scheduler.DAGScheduler: ResultStage 20 (collect at AnalysisRunner.scala:326) finished in 0.145 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,743 INFO scheduler.DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,744 INFO cluster.YarnScheduler: Killing all running tasks in stage 20: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,745 INFO scheduler.DAGScheduler: Job 14 finished: collect at AnalysisRunner.scala:326, took 0.151689 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,873 INFO codegen.CodeGenerator: Code generated in 24.669447 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,912 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,914 INFO scheduler.DAGScheduler: Got job 15 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,914 INFO scheduler.DAGScheduler: Final stage: ResultStage 21 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,914 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,915 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,915 INFO scheduler.DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[90] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,921 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 39.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,923 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,924 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.2.249.196:37057 (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,925 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,925 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[90] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,926 INFO cluster.YarnScheduler: Adding task set 21.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,927 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:15,939 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:38941 (size: 16.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:16,973 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 16) in 1046 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:16,973 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:16,974 INFO scheduler.DAGScheduler: ResultStage 21 (treeReduce at KLLRunner.scala:107) finished in 1.057 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:16,974 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:16,974 INFO cluster.YarnScheduler: Killing all running tasks in stage 21: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:16,975 INFO scheduler.DAGScheduler: Job 15 finished: treeReduce at KLLRunner.scala:107, took 1.062787 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,167 INFO codegen.CodeGenerator: Code generated in 47.267958 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,173 INFO scheduler.DAGScheduler: Registering RDD 95 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,174 INFO scheduler.DAGScheduler: Got map stage job 16 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,174 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,174 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,174 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,175 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[95] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,178 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 75.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,180 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 23.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,180 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.2.249.196:37057 (size: 23.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,180 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,181 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[95] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,181 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,182 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,200 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:38941 (size: 23.9 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,533 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 351 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,534 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,534 INFO scheduler.DAGScheduler: ShuffleMapStage 22 (collect at AnalysisRunner.scala:326) finished in 0.358 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,535 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,535 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,535 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,536 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,607 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,608 INFO scheduler.DAGScheduler: Got job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,608 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,608 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,609 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,609 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[98] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,615 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 66.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,618 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,621 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.2.249.196:37057 (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,622 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,622 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[98] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,623 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,625 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 18) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,639 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:38941 (size: 19.2 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,647 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,655 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 18) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,655 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,656 INFO scheduler.DAGScheduler: ResultStage 24 (collect at AnalysisRunner.scala:326) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,657 INFO scheduler.DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,657 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,658 INFO scheduler.DAGScheduler: Job 17 finished: collect at AnalysisRunner.scala:326, took 0.050277 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,871 INFO scheduler.DAGScheduler: Registering RDD 103 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,871 INFO scheduler.DAGScheduler: Got map stage job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,871 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 25 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,871 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,872 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,872 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,877 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 84.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,879 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,906 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.2.249.196:37057 (size: 27.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,907 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,910 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,910 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,912 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:17,930 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:38941 (size: 27.6 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,325 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 19) in 413 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,325 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,326 INFO scheduler.DAGScheduler: ShuffleMapStage 25 (collect at AnalysisRunner.scala:326) finished in 0.453 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,326 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,326 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,326 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,326 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,385 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,386 INFO scheduler.DAGScheduler: Got job 19 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,387 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,388 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,388 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,390 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[106] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,406 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 169.6 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,409 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 46.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,410 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.2.249.196:37057 (size: 46.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,415 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,415 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[106] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,416 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,419 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,439 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:38941 (size: 46.5 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,452 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,540 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 20) in 121 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,540 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,541 INFO scheduler.DAGScheduler: ResultStage 27 (collect at AnalysisRunner.scala:326) finished in 0.150 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,543 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,545 INFO cluster.YarnScheduler: Killing all running tasks in stage 27: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,546 INFO scheduler.DAGScheduler: Job 19 finished: collect at AnalysisRunner.scala:326, took 0.159804 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,794 INFO codegen.CodeGenerator: Code generated in 35.785974 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,865 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,867 INFO scheduler.DAGScheduler: Got job 20 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,867 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,867 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,867 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,868 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[116] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,879 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 39.7 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,881 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,881 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.2.249.196:37057 (size: 16.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,882 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,883 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[116] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,883 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,888 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:18,900 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:38941 (size: 16.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:19,857 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 21) in 969 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:19,857 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:19,858 INFO scheduler.DAGScheduler: ResultStage 28 (treeReduce at KLLRunner.scala:107) finished in 0.990 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:19,859 INFO scheduler.DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:19,859 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:19,859 INFO scheduler.DAGScheduler: Job 20 finished: treeReduce at KLLRunner.scala:107, took 0.993404 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:19,958 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.2.249.196:37057 in memory (size: 16.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:19,966 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:38941 in memory (size: 16.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,001 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:38941 in memory (size: 27.6 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,003 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.2.249.196:37057 in memory (size: 27.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,020 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.2.249.196:37057 in memory (size: 27.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,030 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:38941 in memory (size: 27.6 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,047 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.2.249.196:37057 in memory (size: 16.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,050 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:38941 in memory (size: 16.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,067 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:38941 in memory (size: 46.5 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,068 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.2.249.196:37057 in memory (size: 46.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,076 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.2.249.196:37057 in memory (size: 46.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,086 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:38941 in memory (size: 46.4 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,102 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.2.249.196:37057 in memory (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,105 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:38941 in memory (size: 19.2 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,112 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.2.249.196:37057 in memory (size: 23.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,114 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:38941 in memory (size: 23.9 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,124 INFO codegen.CodeGenerator: Code generated in 75.483536 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,135 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,135 INFO scheduler.DAGScheduler: Got map stage job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,135 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,135 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,136 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,136 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,140 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 75.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,142 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,143 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.2.249.196:37057 (size: 23.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,143 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,144 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,144 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,145 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 22) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,157 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:38941 (size: 23.8 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,473 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 22) in 328 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,473 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,474 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.336 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,474 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,474 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,474 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,474 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,560 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,561 INFO scheduler.DAGScheduler: Got job 22 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,562 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,562 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,562 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,563 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,568 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 66.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,571 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 19.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,572 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.2.249.196:37057 (size: 19.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,575 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,575 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,575 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,577 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 23) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,588 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:38941 (size: 19.2 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,593 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,599 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 23) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,599 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,600 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.036 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,602 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,602 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,602 INFO scheduler.DAGScheduler: Job 22 finished: collect at AnalysisRunner.scala:326, took 0.042125 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,785 INFO scheduler.DAGScheduler: Registering RDD 129 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,786 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,786 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 32 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,786 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,786 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,786 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[129] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,789 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 54.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,790 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 20.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,790 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.2.249.196:37057 (size: 20.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,791 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,791 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[129] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,791 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,792 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 24) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:20,801 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:38941 (size: 20.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,030 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 24) in 238 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,031 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,031 INFO scheduler.DAGScheduler: ShuffleMapStage 32 (collect at AnalysisRunner.scala:326) finished in 0.244 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,031 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,031 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,032 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,032 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,058 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,059 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,060 INFO scheduler.DAGScheduler: Final stage: ResultStage 34 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,060 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,060 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,060 INFO scheduler.DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[132] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,063 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 90.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,065 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 29.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,065 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.2.249.196:37057 (size: 29.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,066 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,066 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[132] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,066 INFO cluster.YarnScheduler: Adding task set 34.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,067 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 25) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,078 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:38941 (size: 29.1 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,087 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,167 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 25) in 100 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,167 INFO cluster.YarnScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,168 INFO scheduler.DAGScheduler: ResultStage 34 (collect at AnalysisRunner.scala:326) finished in 0.107 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,168 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,168 INFO cluster.YarnScheduler: Killing all running tasks in stage 34: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,169 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.111060 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,186 INFO codegen.CodeGenerator: Code generated in 14.855488 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,264 INFO codegen.CodeGenerator: Code generated in 14.922494 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,306 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,307 INFO scheduler.DAGScheduler: Got job 25 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,308 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,308 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,308 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,309 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,315 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 36.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,317 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,318 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.2.249.196:37057 (size: 16.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,318 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,319 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,319 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,321 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4956 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,332 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:38941 (size: 16.3 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,901 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 26) in 581 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,902 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,902 INFO scheduler.DAGScheduler: ResultStage 35 (treeReduce at KLLRunner.scala:107) finished in 0.592 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,903 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,903 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:21,903 INFO scheduler.DAGScheduler: Job 25 finished: treeReduce at KLLRunner.scala:107, took 0.596791 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,077 INFO codegen.CodeGenerator: Code generated in 37.354312 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,082 INFO scheduler.DAGScheduler: Registering RDD 147 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,082 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,082 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 36 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,082 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,083 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,083 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[147] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,087 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 44.4 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,088 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,088 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.2.249.196:37057 (size: 16.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,089 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,089 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[147] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,089 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,090 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 27) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,102 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:38941 (size: 16.8 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,228 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 27) in 138 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,228 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,229 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (collect at AnalysisRunner.scala:326) finished in 0.145 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,229 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,230 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,230 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,230 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,273 INFO codegen.CodeGenerator: Code generated in 19.804656 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,284 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,286 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,286 INFO scheduler.DAGScheduler: Final stage: ResultStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,286 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 37)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,287 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,287 INFO scheduler.DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[150] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,289 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 32.6 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,291 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,292 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.2.249.196:37057 (size: 11.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,292 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,293 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[150] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,293 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,294 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 28) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,318 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:38941 (size: 11.1 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,322 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,359 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 28) in 65 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,359 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,360 INFO scheduler.DAGScheduler: ResultStage 38 (collect at AnalysisRunner.scala:326) finished in 0.072 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,360 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,360 INFO cluster.YarnScheduler: Killing all running tasks in stage 38: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,360 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.075617 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,383 INFO codegen.CodeGenerator: Code generated in 17.148046 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,582 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,608 INFO codegen.CodeGenerator: Code generated in 9.614644 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,613 INFO scheduler.DAGScheduler: Registering RDD 155 (count at StatsGenerator.scala:66) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,614 INFO scheduler.DAGScheduler: Got map stage job 28 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,614 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 39 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,614 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,614 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,616 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 39 (MapPartitionsRDD[155] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,621 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 24.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,622 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 10.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,623 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.2.249.196:37057 (size: 10.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,624 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,624 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 39 (MapPartitionsRDD[155] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,624 INFO cluster.YarnScheduler: Adding task set 39.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,625 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 29) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4945 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,647 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:38941 (size: 10.8 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,699 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 29) in 74 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,699 INFO cluster.YarnScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,699 INFO scheduler.DAGScheduler: ShuffleMapStage 39 (count at StatsGenerator.scala:66) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,700 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,700 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,700 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,700 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,713 INFO codegen.CodeGenerator: Code generated in 6.738128 ms\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,722 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,723 INFO scheduler.DAGScheduler: Got job 29 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,723 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,723 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 40)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,723 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,723 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[158] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,725 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 11.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,726 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,727 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.2.249.196:37057 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,727 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,728 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[158] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,728 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,729 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 30) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,739 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:38941 (size: 5.5 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,743 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.2.249.196:44990\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,757 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 30) in 28 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,757 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,758 INFO scheduler.DAGScheduler: ResultStage 41 (count at StatsGenerator.scala:66) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,758 INFO scheduler.DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,758 INFO cluster.YarnScheduler: Killing all running tasks in stage 41: Stage finished\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:22,758 INFO scheduler.DAGScheduler: Job 29 finished: count at StatsGenerator.scala:66, took 0.036018 s\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,577 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.2.249.196:37057 in memory (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,578 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:38941 in memory (size: 5.5 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,604 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:38941 in memory (size: 29.1 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,607 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.2.249.196:37057 in memory (size: 29.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,636 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:38941 in memory (size: 10.8 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,637 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.2.249.196:37057 in memory (size: 10.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,649 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.2.249.196:37057 in memory (size: 19.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,656 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:38941 in memory (size: 19.2 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,660 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.2.249.196:37057 in memory (size: 16.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,665 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:38941 in memory (size: 16.3 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,671 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.2.249.196:37057 in memory (size: 20.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,675 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:38941 in memory (size: 20.7 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,690 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.2.249.196:37057 in memory (size: 23.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,694 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:38941 in memory (size: 23.8 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,697 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.2.249.196:37057 in memory (size: 11.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,699 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:38941 in memory (size: 11.1 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,710 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.2.249.196:37057 in memory (size: 16.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,712 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:38941 in memory (size: 16.8 KiB, free: 5.7 GiB)\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,734 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,753 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,778 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,779 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,791 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,809 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,859 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,859 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,877 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,887 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,932 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,932 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,932 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,937 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,938 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ec093cab-be47-46d8-ace3-8804cbe9c093\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:23,942 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-da70237d-4b3e-4790-850f-0760aba690c1\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:24,053 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2026-02-17 22:32:24,054 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: dq-20260217-215414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:\n",
      "  statistics: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/monitoring/baselines/data-quality/statistics.json\n",
      "  constraints: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/monitoring/baselines/data-quality/constraints.json\n",
      "Monitoring:\n",
      "  schedule: dq-20260217-215414\n",
      "  reports:  s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/monitoring/reports/data-quality/\n",
      "CloudWatch alarms created (latency p95, 5XX errors).\n"
     ]
    }
   ],
   "source": [
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=50,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "baseline_prefix = f\"{S3_MON}baselines/data-quality/\"\n",
    "report_prefix   = f\"{S3_MON}reports/data-quality/\"\n",
    "\n",
    "statistics_path  = f\"{baseline_prefix}statistics.json\"\n",
    "constraints_path = f\"{baseline_prefix}constraints.json\"\n",
    "\n",
    "S3_TRAIN_FE_HEADER = f\"{S3_PREP}{job_tag}/train_fe_header.csv\"\n",
    "s3_put_bytes(df_train_fe[[\"class\"] + feature_cols].to_csv(index=False, header=True).encode(\"utf-8\"),\n",
    "             S3_TRAIN_FE_HEADER, \"text/csv\")\n",
    "\n",
    "monitor.suggest_baseline(\n",
    "    baseline_dataset=S3_TRAIN_FE_HEADER,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_prefix,\n",
    "    wait=True,\n",
    ")\n",
    "\n",
    "dq_schedule_name = f\"dq-{job_tag}\"[:63]\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=dq_schedule_name,\n",
    "    endpoint_input=endpoint_name,\n",
    "    output_s3_uri=report_prefix,\n",
    "    statistics=statistics_path,\n",
    "    constraints=constraints_path,\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")\n",
    "\n",
    "print(\"Baseline:\")\n",
    "print(\"  statistics:\", statistics_path)\n",
    "print(\"  constraints:\", constraints_path)\n",
    "print(\"Monitoring:\")\n",
    "print(\"  schedule:\", dq_schedule_name)\n",
    "print(\"  reports: \", report_prefix)\n",
    "\n",
    "def put_alarm(metric_name: str, threshold: float, period: int, eval_periods: int,\n",
    "              alarm_name: str, stat: str = None, ext_stat: str = None,\n",
    "              comparison: str = \"GreaterThanThreshold\"):\n",
    "    if (stat is None) == (ext_stat is None):\n",
    "        raise ValueError(\"Provide exactly one of stat or ext_stat\")\n",
    "\n",
    "    kwargs = dict(\n",
    "        AlarmName=alarm_name,\n",
    "        Namespace=\"AWS/SageMaker\",\n",
    "        MetricName=metric_name,\n",
    "        Dimensions=[\n",
    "            {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "            {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"},\n",
    "        ],\n",
    "        Period=period,\n",
    "        EvaluationPeriods=eval_periods,\n",
    "        Threshold=float(threshold),\n",
    "        ComparisonOperator=comparison,\n",
    "        TreatMissingData=\"notBreaching\",\n",
    "    )\n",
    "    if stat:\n",
    "        kwargs[\"Statistic\"] = stat\n",
    "    else:\n",
    "        kwargs[\"ExtendedStatistic\"] = ext_stat\n",
    "\n",
    "    cw.put_metric_alarm(**kwargs)\n",
    "\n",
    "put_alarm(\n",
    "    metric_name=\"ModelLatency\",\n",
    "    ext_stat=\"p95.0\",\n",
    "    threshold=500,          \n",
    "    period=60,\n",
    "    eval_periods=5,\n",
    "    alarm_name=f\"{endpoint_name}-latency-p95\",\n",
    ")\n",
    "\n",
    "put_alarm(\n",
    "    metric_name=\"Invocation5XXErrors\",\n",
    "    stat=\"Sum\",\n",
    "    threshold=1,\n",
    "    period=60,\n",
    "    eval_periods=1,\n",
    "    alarm_name=f\"{endpoint_name}-5xx\",\n",
    ")\n",
    "\n",
    "print(\"CloudWatch alarms created (latency p95, 5XX errors).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570258d3-9432-47d8-aeca-f2525f2e2f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CSV S3: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/prepared/20260217-235416//train_fe_header.csv\n",
      "Baseline prefix: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/monitoring/baselines/data-quality/20260217-235416/\n",
      "Report prefix: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/monitoring/reports/data-quality/20260217-235416/\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"xgb-ep1-20260217-215414\"\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = \"sagemaker-us-east-1-579568333234\"\n",
    "prefix = \"sagemaker-featurestore-demo\"\n",
    "S3_MON = f\"s3://{bucket}/{prefix}/monitoring/\"\n",
    "\n",
    "job_tag = datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "baseline_prefix = f\"{S3_MON}baselines/data-quality/{job_tag}/\"\n",
    "report_prefix   = f\"{S3_MON}reports/data-quality/{job_tag}/\"\n",
    "\n",
    "baseline_local = \"train_fe_header.csv\"\n",
    "baseline_df = df_train_fe[[\"class\"] + feature_cols].copy()\n",
    "\n",
    "baseline_df = baseline_df.loc[:, ~baseline_df.columns.duplicated()]\n",
    "baseline_df.to_csv(baseline_local, index=False, header=True)\n",
    "\n",
    "baseline_s3 = S3Uploader.upload(baseline_local, f\"s3://{bucket}/{prefix}/prepared/{job_tag}/\")\n",
    "print(\"Baseline CSV S3:\", baseline_s3)\n",
    "print(\"Baseline prefix:\", baseline_prefix)\n",
    "print(\"Report prefix:\", report_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0493f9fc-5644-48f5-b9e0-4347c962a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1763/96233992.py:27: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  run_id = f\"{datetime.utcnow():%Y%m%d-%H%M%S}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline uploaded: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/clarify/20260218-000315/baseline/baseline.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.clarify:Analysis Config: {'dataset_type': 'text/csv', 'headers': ['class', 'redshift', 'alpha', 'delta', 'u', 'g', 'r', 'i', 'z', 'u_g', 'u_r', 'u_i', 'u_z', 'g_r', 'g_i', 'g_z', 'r_i', 'r_z', 'i_z', 'mean_mag', 'mag_std', 'mag_span', 'redshift_bin'], 'label': 'class', 'predicted_label_dataset_uri': 's3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/clarify/20260218-000315/baseline/predicted_label.csv', 'predicted_label_headers': ['predicted_label'], 'predicted_label': 'predicted_label', 'label_values_or_threshold': ['STAR'], 'facet': [{'name_or_index': 'redshift_bin'}], 'methods': {'report': {'name': 'report', 'title': 'Analysis Report'}, 'pre_training_bias': {'methods': 'all'}}}\n",
      "INFO:sagemaker:Creating processing-job with name Clarify-Bias-2026-02-18-00-03-15-829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted-label uploaded: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/clarify/20260218-000315/baseline/predicted_label.csv\n",
      "....................\u001b[34msagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\u001b[0m\n",
      "\u001b[34msagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\u001b[0m\n",
      "\u001b[34mWe are not in a supported iso region, /bin/sh exiting gracefully with no changes.\u001b[0m\n",
      "\u001b[34mWARNING:root:logging.conf not found when configuring logging, using default logging configuration.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Starting SageMaker Clarify Processing job\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Analysis config path: /opt/ml/processing/input/config/analysis_config.json\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Analysis result path: /opt/ml/processing/output\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:This host is algo-1.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:This host is the leader.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Number of hosts in the cluster is 1.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Running Python / Pandas based analyzer.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_factory:Dataset type: text/csv uri: /opt/ml/processing/input/data\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Loading dataset...\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Loaded dataset. Dataset info:\u001b[0m\n",
      "\u001b[34m<class 'pandas.core.frame.DataFrame'>\u001b[0m\n",
      "\u001b[34mRangeIndex: 2000 entries, 0 to 1999\u001b[0m\n",
      "\u001b[34mData columns (total 22 columns):\n",
      " #   Column        Non-Null Count  Dtype  \u001b[0m\n",
      "\u001b[34m---  ------        --------------  -----  \n",
      " 0   redshift      2000 non-null   float64\n",
      " 1   alpha         2000 non-null   float64\n",
      " 2   delta         2000 non-null   float64\n",
      " 3   u             2000 non-null   float64\n",
      " 4   g             2000 non-null   float64\n",
      " 5   r             2000 non-null   float64\n",
      " 6   i             2000 non-null   float64\n",
      " 7   z             2000 non-null   float64\n",
      " 8   u_g           2000 non-null   float64\n",
      " 9   u_r           2000 non-null   float64\n",
      " 10  u_i           2000 non-null   float64\n",
      " 11  u_z           2000 non-null   float64\n",
      " 12  g_r           2000 non-null   float64\n",
      " 13  g_i           2000 non-null   float64\n",
      " 14  g_z           2000 non-null   float64\n",
      " 15  r_i           2000 non-null   float64\n",
      " 16  r_z           2000 non-null   float64\n",
      " 17  i_z           2000 non-null   float64\n",
      " 18  mean_mag      2000 non-null   float64\n",
      " 19  mag_std       2000 non-null   float64\n",
      " 20  mag_span      2000 non-null   float64\n",
      " 21  redshift_bin  2000 non-null   object \u001b[0m\n",
      "\u001b[34mdtypes: float64(21), object(1)\u001b[0m\n",
      "\u001b[34mmemory usage: 343.9+ KB\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Calculated global analysis with predictor\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:=====================================\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Calculating pre-training bias metrics\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:=====================================\u001b[0m\n",
      "\u001b[34mINFO:smclarify.bias.metrics.common:Column class with data uniqueness fraction 0.0015 is classifed as a CATEGORICAL column\u001b[0m\n",
      "\u001b[34mINFO:smclarify.bias.metrics.common:Column redshift_bin with data uniqueness fraction 0.002 is classifed as a CATEGORICAL column\u001b[0m\n",
      "\u001b[34mINFO:smclarify.bias.metrics.common:Column class with data uniqueness fraction 0.0015 is classifed as a CATEGORICAL column\u001b[0m\n",
      "\u001b[34mERROR:smclarify.bias.report:CDDL metrics failed\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/report.py\", line 322, in _categorical_metric_call_wrapper\n",
      "    metric_value = smclarify.bias.metrics.call_metric(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/__init__.py\", line 27, in call_metric\n",
      "    return metric(**{key: kwargs[key] for key in inspect.signature(metric).parameters.keys()})\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/pretraining.py\", line 194, in CDDL\n",
      "    return common.CDD(feature, sensitive_facet_index, positive_label_index, group_variable)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/common.py\", line 99, in CDD\n",
      "    raise ValueError(\"Group variable is empty or not provided\")\u001b[0m\n",
      "\u001b[34mValueError: Group variable is empty or not provided\u001b[0m\n",
      "\u001b[34mERROR:smclarify.bias.report:CDDL metrics failed\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/report.py\", line 322, in _categorical_metric_call_wrapper\n",
      "    metric_value = smclarify.bias.metrics.call_metric(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/__init__.py\", line 27, in call_metric\n",
      "    return metric(**{key: kwargs[key] for key in inspect.signature(metric).parameters.keys()})\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/pretraining.py\", line 194, in CDDL\n",
      "    return common.CDD(feature, sensitive_facet_index, positive_label_index, group_variable)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/common.py\", line 99, in CDD\n",
      "    raise ValueError(\"Group variable is empty or not provided\")\u001b[0m\n",
      "\u001b[34mValueError: Group variable is empty or not provided\u001b[0m\n",
      "\u001b[34mERROR:smclarify.bias.report:CDDL metrics failed\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/report.py\", line 322, in _categorical_metric_call_wrapper\n",
      "    metric_value = smclarify.bias.metrics.call_metric(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/__init__.py\", line 27, in call_metric\n",
      "    return metric(**{key: kwargs[key] for key in inspect.signature(metric).parameters.keys()})\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/pretraining.py\", line 194, in CDDL\n",
      "    return common.CDD(feature, sensitive_facet_index, positive_label_index, group_variable)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/common.py\", line 99, in CDD\n",
      "    raise ValueError(\"Group variable is empty or not provided\")\u001b[0m\n",
      "\u001b[34mValueError: Group variable is empty or not provided\u001b[0m\n",
      "\u001b[34mERROR:smclarify.bias.report:CDDL metrics failed\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/report.py\", line 322, in _categorical_metric_call_wrapper\n",
      "    metric_value = smclarify.bias.metrics.call_metric(\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/__init__.py\", line 27, in call_metric\n",
      "    return metric(**{key: kwargs[key] for key in inspect.signature(metric).parameters.keys()})\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/pretraining.py\", line 194, in CDDL\n",
      "    return common.CDD(feature, sensitive_facet_index, positive_label_index, group_variable)\n",
      "  File \"/usr/local/lib/python3.9/site-packages/smclarify/bias/metrics/common.py\", line 99, in CDD\n",
      "    raise ValueError(\"Group variable is empty or not provided\")\u001b[0m\n",
      "\u001b[34mValueError: Group variable is empty or not provided\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:======================================\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Calculating bias statistics for report\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:======================================\u001b[0m\n",
      "\u001b[34mINFO:smclarify.bias.metrics.common:Column class with data uniqueness fraction 0.0015 is classifed as a CATEGORICAL column\u001b[0m\n",
      "\u001b[34mINFO:smclarify.bias.metrics.common:Column redshift_bin with data uniqueness fraction 0.002 is classifed as a CATEGORICAL column\u001b[0m\n",
      "\u001b[34mINFO:smclarify.bias.metrics.common:Column class with data uniqueness fraction 0.0015 is classifed as a CATEGORICAL column\u001b[0m\n",
      "\u001b[34mINFO:smclarify.bias.metrics.common:Column class with data uniqueness fraction 0.0015 is classifed as a CATEGORICAL column\u001b[0m\n",
      "\u001b[34mINFO:smclarify.bias.metrics.common:Column predicted_label with data uniqueness fraction 0.0015 is classifed as a CATEGORICAL column\u001b[0m\n",
      "\u001b[34mINFO:analyzer.utils.spark_util:Converting Pandas DataFrame to SparkDataFrame for computing report metadata\u001b[0m\n",
      "\u001b[34mSetting default log level to \"WARN\".\u001b[0m\n",
      "\u001b[34mTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\u001b[0m\n",
      "\u001b[34m26/02/18 00:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m#015[Stage 0:>                                                          (0 + 4) / 4]#015#015                                                                                #015INFO:sagemaker-clarify-processing:Calculated global analysis without predictor\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Collected analyses: \u001b[0m\n",
      "\u001b[34m{'version': '1.0', 'pre_training_bias_metrics': {'label': 'class', 'facets': {'redshift_bin': [{'value_or_threshold': 'high', 'metrics': [{'name': 'CDDL', 'description': 'Conditional Demographic Disparity in Labels (CDDL)', 'value': None, 'error': 'Group variable is empty or not provided'}, {'name': 'CI', 'description': 'Class Imbalance (CI)', 'value': 0.5}, {'name': 'DPL', 'description': 'Difference in Positive Proportions in Labels (DPL)', 'value': 0.2793333333333333}, {'name': 'JS', 'description': 'Jensen-Shannon Divergence (JS)', 'value': 0.011387083684970822}, {'name': 'KL', 'description': 'Kullback-Liebler Divergence (KL)', 'value': -0.23607495571775441}, {'name': 'KS', 'description': 'Kolmogorov-Smirnov Distance (KS)', 'value': 0.2793333333333333}, {'name': 'LP', 'description': 'L-p Norm (LP)', 'value': 0.2793333333333333}, {'name': 'TVD', 'description': 'Total Variation Distance (TVD)', 'value': 0.13966666666666666}]}, {'value_or_threshold': 'low', 'metrics': [{'name': 'CDDL', 'description': 'Conditional Demographic Disparity in Labels (CDDL)', 'value': None, 'error': 'Group variable is empty or not provided'}, {'name': 'CI', 'description': 'Class Imbalance (CI)', 'value': 0.5}, {'name': 'DPL', 'description': 'Difference in Positive Proportions in Labels (DPL)', 'value': 0.2793333333333333}, {'name': 'JS', 'description': 'Jensen-Shannon Divergence (JS)', 'value': 0.011387083684970822}, {'name': 'KL', 'description': 'Kullback-Liebler Divergence (KL)', 'value': -0.23607495571775441}, {'name': 'KS', 'description': 'Kolmogorov-Smirnov Distance (KS)', 'value': 0.2793333333333333}, {'name': 'LP', 'description': 'L-p Norm (LP)', 'value': 0.2793333333333333}, {'name': 'TVD', 'description': 'Total Variation Distance (TVD)', 'value': 0.13966666666666666}]}, {'value_or_threshold': 'very_low', 'metrics': [{'name': 'CDDL', 'description': 'Conditional Demographic Disparity in Labels (CDDL)', 'value': None, 'error': 'Group variable is empty or not provided'}, {'name': 'CI', 'description': 'Class Imbalance (CI)', 'value': 0.5}, {'name': 'DPL', 'description': 'Difference in Positive Proportions in Labels (DPL)', 'value': -0.838}, {'name': 'JS', 'description': 'Jensen-Shannon Divergence (JS)', 'value': 0.16805275291393118}, {'name': 'KL', 'description': 'Kullback-Liebler Divergence (KL)', 'value': 1.820158943749753}, {'name': 'KS', 'description': 'Kolmogorov-Smirnov Distance (KS)', 'value': 0.838}, {'name': 'LP', 'description': 'L-p Norm (LP)', 'value': 0.838}, {'name': 'TVD', 'description': 'Total Variation Distance (TVD)', 'value': 0.419}]}, {'value_or_threshold': 'very_high', 'metrics': [{'name': 'CDDL', 'description': 'Conditional Demographic Disparity in Labels (CDDL)', 'value': None, 'error': 'Group variable is empty or not provided'}, {'name': 'CI', 'description': 'Class Imbalance (CI)', 'value': 0.5}, {'name': 'DPL', 'description': 'Difference in Positive Proportions in Labels (DPL)', 'value': 0.2793333333333333}, {'name': 'JS', 'description': 'Jensen-Shannon Divergence (JS)', 'value': 0.011387083684970822}, {'name': 'KL', 'description': 'Kullback-Liebler Divergence (KL)', 'value': -0.23607495571775441}, {'name': 'KS', 'description': 'Kolmogorov-Smirnov Distance (KS)', 'value': 0.2793333333333333}, {'name': 'LP', 'description': 'L-p Norm (LP)', 'value': 0.2793333333333333}, {'name': 'TVD', 'description': 'Total Variation Distance (TVD)', 'value': 0.13966666666666666}]}]}, 'label_value_or_threshold': 'STAR'}}\u001b[0m\n",
      "\u001b[34mINFO:analyzer.utils.util:['jupyter', 'nbconvert', '--to', 'html', '--output', '/opt/ml/processing/output/report.html', '/opt/ml/processing/output/report.ipynb', '--template', 'sagemaker-xai']\u001b[0m\n",
      "\u001b[34m[NbConvertApp] Converting notebook /opt/ml/processing/output/report.ipynb to html\u001b[0m\n",
      "\u001b[34m[NbConvertApp] Writing 689516 bytes to /opt/ml/processing/output/report.html\u001b[0m\n",
      "\u001b[34mINFO:analyzer.utils.util:['wkhtmltopdf', '-q', '--enable-local-file-access', '/opt/ml/processing/output/report.html', '/opt/ml/processing/output/report.pdf']\u001b[0m\n",
      "\u001b[34mINFO:analyzer.utils.system_util:exit_message: Completed: SageMaker XAI Analyzer ran successfully\u001b[0m\n",
      "\u001b[34mINFO:py4j.clientserver:Closing down clientserver connection\u001b[0m\n",
      "\n",
      "Downloaded: analysis.json, report.pdf\n",
      "Reports S3: s3://sagemaker-us-east-1-579568333234/sagemaker-featurestore-demo/clarify/20260218-000315/reports\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"xgb-ep1-20260217-215414\"\n",
    "bucket = \"sagemaker-us-east-1-579568333234\"\n",
    "prefix = \"sagemaker-featurestore-demo\"\n",
    "\n",
    "label_col = \"class\"\n",
    "facet_col = \"redshift\"\n",
    "positive_label = \"STAR\"\n",
    "\n",
    "session = Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "run_id = f\"{datetime.utcnow():%Y%m%d-%H%M%S}\"\n",
    "baseline_s3_key = f\"s3://{bucket}/{prefix}/clarify/{run_id}/baseline\"\n",
    "reports_s3_key  = f\"s3://{bucket}/{prefix}/clarify/{run_id}/reports\"\n",
    "\n",
    "def make_unique_columns(cols):\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for c in map(str, cols):\n",
    "        if c not in seen:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}__dup{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "baseline_df = df_train_fe[[label_col, facet_col] + feature_cols].copy()\n",
    "baseline_df = baseline_df.loc[:, ~baseline_df.columns.duplicated()]\n",
    "\n",
    "if baseline_df.columns.duplicated().any():\n",
    "    baseline_df.columns = make_unique_columns(baseline_df.columns)\n",
    "\n",
    "baseline_df = baseline_df.sample(\n",
    "    n=min(2000, len(baseline_df)),\n",
    "    random_state=42\n",
    ").reset_index(drop=True)\n",
    "\n",
    "baseline_df[\"redshift_bin\"] = pd.qcut(\n",
    "    baseline_df[\"redshift\"],\n",
    "    q=4,\n",
    "    labels=[\"very_low\", \"low\", \"high\", \"very_high\"]\n",
    ")\n",
    "\n",
    "facet_col_for_bias = \"redshift_bin\"\n",
    "\n",
    "feature_cols_clean = [c for c in feature_cols if c in baseline_df.columns]\n",
    "\n",
    "X = (\n",
    "    baseline_df[feature_cols_clean]\n",
    "    .apply(pd.to_numeric, errors=\"coerce\")\n",
    "    .fillna(0.0)\n",
    "    .to_numpy(dtype=float)\n",
    ")\n",
    "\n",
    "\n",
    "baseline_local = \"baseline.csv\"\n",
    "baseline_df.to_csv(baseline_local, index=False, header=True)\n",
    "\n",
    "baseline_full_uri = S3Uploader.upload(baseline_local, baseline_s3_key)\n",
    "\n",
    "headers = list(baseline_df.columns)\n",
    "if len(headers) != len(set(headers)):\n",
    "    raise RuntimeError(\"Duplicate headers detected.\")\n",
    "\n",
    "print(\"Baseline uploaded:\", baseline_full_uri)\n",
    "\n",
    "\n",
    "pred = Predictor(endpoint_name=endpoint_name, sagemaker_session=session)\n",
    "pred.serializer = CSVSerializer()\n",
    "pred.deserializer = JSONDeserializer()\n",
    "\n",
    "batch_size = 200\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(0, len(X), batch_size):\n",
    "    chunk = X[i:i+batch_size]\n",
    "    payload = \"\\n\".join(\",\".join(map(str, row)) for row in chunk)\n",
    "\n",
    "    resp = pred.predict(payload)\n",
    "\n",
    "    preds = resp[\"predictions\"]\n",
    "    probs = np.asarray([p[\"score\"] for p in preds], dtype=float)\n",
    "    ids = np.argmax(probs, axis=1)\n",
    "\n",
    "    pred_labels.extend([ID_TO_LABEL[int(j)] for j in ids])\n",
    "\n",
    "predicted_df = pd.DataFrame({\"predicted_label\": pred_labels})\n",
    "\n",
    "pred_local = \"predicted_label.csv\"\n",
    "predicted_df.to_csv(pred_local, index=False, header=False)\n",
    "\n",
    "predicted_full_uri = S3Uploader.upload(pred_local, baseline_s3_key)\n",
    "\n",
    "print(\"Predicted-label uploaded:\", predicted_full_uri)\n",
    "\n",
    "\n",
    "clarify_processor = clarify.SageMakerClarifyProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "data_config = clarify.DataConfig(\n",
    "    s3_data_input_path=baseline_full_uri,\n",
    "    s3_output_path=reports_s3_key,\n",
    "    label=label_col,\n",
    "    headers=headers,\n",
    "    dataset_type=\"text/csv\",\n",
    "    predicted_label_dataset_uri=predicted_full_uri,\n",
    "    predicted_label_headers=[\"predicted_label\"],\n",
    "    predicted_label=\"predicted_label\",\n",
    ")\n",
    "\n",
    "bias_config = clarify.BiasConfig(\n",
    "    label_values_or_threshold=[positive_label],\n",
    "    facet_name=facet_col_for_bias,   # <- categorical bin\n",
    ")\n",
    "\n",
    "clarify_processor.run_bias(\n",
    "    data_config=data_config,\n",
    "    bias_config=bias_config,\n",
    "    pre_training_methods=\"all\",\n",
    "    post_training_methods=None,\n",
    "    wait=True,\n",
    "    logs=True,\n",
    ")\n",
    "\n",
    "\n",
    "S3Downloader.download(f\"{reports_s3_key}/analysis.json\", \".\")\n",
    "S3Downloader.download(f\"{reports_s3_key}/report.pdf\", \".\")\n",
    "\n",
    "print(\"Downloaded: analysis.json, report.pdf\")\n",
    "print(\"Reports S3:\", reports_s3_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35d261b8-530c-4e8a-9782-2f1e5c5577f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"700\"\n",
       "            src=\"report.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe9ba63dc70>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"report.pdf\", width=1000, height=700)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
